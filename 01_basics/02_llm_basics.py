#!/usr/bin/env python3
"""
ğŸ§  02_llm_basics.py - Deep Dive into LLMs and Chat Models

Learning Outcomes:
After completing this tutorial, you'll understand:
- Difference between LLMs and Chat Models (and when to use which!)
- How temperature affects AI creativity
- Token limits and why they matter  
- Different model providers and their strengths
- How to handle API errors like a pro

Bhai, LLMs are the heart of LangChain! Let's understand them properly ğŸ’—
Created by your friendly neighborhood developer from tier-2 city ğŸ˜ï¸
"""

import os
import time
from dotenv import load_dotenv

load_dotenv()

try:
    from langchain.llms import OpenAI
    from langchain.chat_models import ChatOpenAI
    from langchain.schema import HumanMessage, AIMessage, SystemMessage
    print("âœ… All imports successful! Ready to explore LLMs!")
except ImportError as e:
    print(f"âŒ Import error: {e}")
    print("Run: pip install -r requirements.txt")
    exit(1)


class LLMExplorer:
    """
    Your friendly LLM exploration class!
    
    Think of this as your guide through the world of Language Models
    Iske saath hum explore karenge LLMs ki duniya! ğŸ—ºï¸
    """
    
    def __init__(self):
        """
        Initialize our LLM explorer
        Setting up our AI companions for the journey!
        """
        # Check API key first
        if not os.getenv("OPENAI_API_KEY"):
            print("âŒ Please set OPENAI_API_KEY in your .env file!")
            exit(1)
        
        print("ğŸš€ LLM Explorer initialized! Let's dive deep into AI models!")
    
    def demonstrate_temperature_effects(self):
        """
        Show how temperature affects AI creativity
        
        Temperature is like chai - too cold (0) = boring, too hot (1) = chaotic
        Perfect temperature (0.7) = just right! ğŸ«–
        """
        print("\n" + "="*60)
        print("ğŸŒ¡ï¸ TEMPERATURE EXPERIMENT - AI CREATIVITY LEVELS")
        print("="*60)
        
        prompt = "Write a tagline for a street food stall in India:"
        temperatures = [0.0, 0.5, 0.9]
        
        for temp in temperatures:
            print(f"\nğŸŒ¡ï¸ Temperature: {temp}")
            if temp == 0.0:
                print("   (Deterministic - Same output every time)")
            elif temp == 0.5:
                print("   (Balanced - Good mix of consistency and creativity)")
            else:
                print("   (Creative - Random and wild responses!)")
            
            try:
                llm = OpenAI(
                    temperature=temp,
                    max_tokens=50,
                    model_name="gpt-3.5-turbo-instruct"
                )
                
                response = llm(prompt)
                print(f"ğŸ¤– Response: {response.strip()}")
                
                # Small delay to avoid rate limits (API ki respect karna chahiye!)
                time.sleep(1)
                
            except Exception as e:
                print(f"âŒ Error at temperature {temp}: {e}")
    
    def llm_vs_chat_comparison(self):
        """
        Compare traditional LLM vs Chat Models
        
        LLM = Your friend who completes your sentences
        Chat = Your friend who has proper conversations
        """
        print("\n" + "="*60)
        print("ğŸ¥Š LLM vs CHAT MODEL - The Ultimate Showdown!")
        print("="*60)
        
        test_prompt = "Explain quantum computing to a 10-year-old"
        
        print("ğŸ”µ ROUND 1: Traditional LLM")
        print("-" * 30)
        try:
            llm = OpenAI(
                temperature=0.7,
                max_tokens=100,
                model_name="gpt-3.5-turbo-instruct"
            )
            
            llm_response = llm(test_prompt)
            print(f"ğŸ“ LLM Response:\n{llm_response.strip()}")
            
        except Exception as e:
            print(f"âŒ LLM Error: {e}")
        
        print("\nğŸ”´ ROUND 2: Chat Model")
        print("-" * 30)
        try:
            chat = ChatOpenAI(
                temperature=0.7,
                max_tokens=100,
                model_name="gpt-3.5-turbo"
            )
            
            message = HumanMessage(content=test_prompt)
            chat_response = chat([message])
            print(f"ğŸ’¬ Chat Response:\n{chat_response.content}")
            
        except Exception as e:
            print(f"âŒ Chat Error: {e}")
        
        print("\nğŸ† WINNER: Generally Chat Models! Why?")
        print("   âœ… Better conversation understanding")
        print("   âœ… Handles context more naturally")
        print("   âœ… Designed for back-and-forth dialogue")
        print("   âœ… Better instruction following")
    
    def demonstrate_system_messages(self):
        """
        Show the power of system messages in chat models
        
        System message = Giving your AI friend a personality/role
        Like telling your friend "Act like a teacher" or "Be funny"
        """
        print("\n" + "="*60)
        print("ğŸ­ SYSTEM MESSAGES - Giving AI Different Personalities")
        print("="*60)
        
        user_question = "What's the best way to learn programming?"
        
        # Different system personalities
        personalities = [
            {
                "name": "Professional Teacher",
                "system": "You are a professional programming instructor with 10 years of experience. Be structured and educational."
            },
            {
                "name": "Friendly Bhai",
                "system": "You are a friendly senior developer from India who loves helping juniors. Use casual language and include some Hindi words naturally."
            },
            {
                "name": "Motivational Coach", 
                "system": "You are an enthusiastic motivational coach who believes everyone can learn programming. Be energetic and encouraging!"
            }
        ]
        
        try:
            chat = ChatOpenAI(
                temperature=0.7,
                max_tokens=120,
                model_name="gpt-3.5-turbo"
            )
            
            for personality in personalities:
                print(f"\nğŸª Personality: {personality['name']}")
                print("-" * 40)
                
                messages = [
                    SystemMessage(content=personality['system']),
                    HumanMessage(content=user_question)
                ]
                
                response = chat(messages)
                print(f"ğŸ¤– Response: {response.content}")
                
                time.sleep(1)  # Be nice to the API
                
        except Exception as e:
            print(f"âŒ Error in personality demo: {e}")
    
    def token_limits_explanation(self):
        """
        Explain tokens and why they matter
        
        Tokens are like words, but not exactly words
        Think of them as the "currency" of AI conversations ğŸ’°
        """
        print("\n" + "="*60)
        print("ğŸª™ TOKENS - The Currency of AI Conversations")
        print("="*60)
        
        explanation = """
        What are Tokens? ğŸ¤”
        
        Tokens â‰  Words exactly!
        
        Examples:
        â€¢ "Hello" = 1 token
        â€¢ "Programming" = 1 token  
        â€¢ "LangChain" = 2 tokens (Lang + Chain)
        â€¢ "AI/ML" = 3 tokens (AI + / + ML)
        â€¢ "ğŸš€" = 1 token (yes, emojis count!)
        
        Why Tokens Matter:
        ğŸ’° Cost: You pay per token used
        ğŸ“ Limits: Models have max token limits
        â±ï¸ Speed: More tokens = slower response
        
        Rough Conversion:
        â€¢ 1 token â‰ˆ 0.75 words (English)
        â€¢ 1 token â‰ˆ 4 characters (average)
        
        Model Token Limits:
        â€¢ GPT-3.5-turbo: 4,096 tokens
        â€¢ GPT-4: 8,192 tokens (some versions: 32k)
        â€¢ GPT-4-turbo: 128,000 tokens
        
        Pro Tips:
        âœ… Keep prompts concise but clear
        âœ… Use max_tokens to control response length
        âœ… Monitor token usage for cost control
        âœ… For long documents, use chunking strategies
        """
        
        print(explanation)
        
        # Demonstrate token counting
        print("\nğŸ§® TOKEN COUNTING DEMO")
        print("-" * 30)
        
        try:
            # Import tiktoken for accurate token counting
            import tiktoken
            
            # Get the tokenizer for GPT-3.5-turbo
            encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
            
            test_texts = [
                "Hello World!",
                "LangChain is awesome!",
                "à¤­à¤¾à¤°à¤¤ à¤®à¥‡à¤‚ AI à¤•à¤¾ à¤­à¤µà¤¿à¤·à¥à¤¯ à¤‰à¤œà¥à¤œà¥à¤µà¤² à¤¹à¥ˆà¥¤",  # Hindi text
                "ğŸš€ Building AI apps with LangChain ğŸ¤–",
                "def hello_world():\n    return 'Hello, LangChain!'"
            ]
            
            for text in test_texts:
                tokens = encoding.encode(text)
                print(f"Text: '{text}'")
                print(f"Token Count: {len(tokens)} tokens")
                print(f"Tokens: {tokens[:10]}{'...' if len(tokens) > 10 else ''}")
                print("-" * 30)
                
        except ImportError:
            print("âš ï¸ tiktoken not installed. Install it for accurate token counting:")
            print("   pip install tiktoken")
    
    def error_handling_demo(self):
        """
        Show how to handle common API errors gracefully
        
        Errors happen - network issues, quota exceeded, invalid keys
        Let's handle them like pros! ğŸ›¡ï¸
        """
        print("\n" + "="*60)
        print("ğŸ›¡ï¸ ERROR HANDLING - Dealing with API Issues Like a Pro")
        print("="*60)
        
        print("Common API Errors and Solutions:")
        print("1. ğŸ”‘ Invalid API Key â†’ Check your .env file")
        print("2. ğŸ’° Quota Exceeded â†’ Add billing info or wait")
        print("3. ğŸŒ Network Issues â†’ Retry with backoff")
        print("4. ğŸ”¢ Token Limit â†’ Reduce input/output size")
        print("5. âš¡ Rate Limit â†’ Add delays between calls")
        
        # Demonstrate proper error handling
        print("\nğŸ”¬ Testing Error Handling:")
        
        try:
            # This might fail if quota is exceeded or network issues
            chat = ChatOpenAI(
                temperature=0.7,
                max_tokens=50,
                model_name="gpt-3.5-turbo",
                request_timeout=10  # 10 second timeout
            )
            
            message = HumanMessage(content="Test message for error handling demo")
            response = chat([message])
            print(f"âœ… Success: {response.content[:100]}...")
            
        except Exception as e:
            error_type = type(e).__name__
            error_msg = str(e)
            
            print(f"âŒ Caught {error_type}: {error_msg}")
            
            # Provide specific solutions based on error type
            if "API key" in error_msg.lower():
                print("ğŸ”§ Solution: Check your OPENAI_API_KEY in .env file")
            elif "quota" in error_msg.lower() or "billing" in error_msg.lower():
                print("ğŸ”§ Solution: Add payment method to your OpenAI account")
            elif "timeout" in error_msg.lower():
                print("ğŸ”§ Solution: Check internet connection or increase timeout")
            elif "rate" in error_msg.lower():
                print("ğŸ”§ Solution: Add delays between API calls")
            else:
                print("ğŸ”§ Solution: Check OpenAI status page and your internet connection")
    
    def model_comparison_guide(self):
        """
        Compare different OpenAI models
        
        Har model ka apna character hai - let's understand them! ğŸ­
        """
        print("\n" + "="*60)
        print("ğŸ† MODEL COMPARISON - Choosing the Right AI for the Job")
        print("="*60)
        
        models_info = """
        ğŸ¤– OpenAI Model Family:
        
        1. ğŸ’° GPT-3.5-turbo (Most Popular!)
           â€¢ Cost: $0.002 per 1K tokens
           â€¢ Speed: Fast âš¡
           â€¢ Use for: Chatbots, basic text generation
           â€¢ Best for: Learning, prototypes, cost-effective apps
        
        2. ğŸ§  GPT-4 (The Smart One!)
           â€¢ Cost: $0.03 per 1K tokens (15x more expensive!)
           â€¢ Speed: Slower ğŸŒ  
           â€¢ Use for: Complex reasoning, coding, analysis
           â€¢ Best for: Production apps that need high quality
        
        3. ğŸ“š GPT-4-turbo (The Balanced One!)
           â€¢ Cost: $0.01 per 1K tokens
           â€¢ Speed: Faster than GPT-4
           â€¢ Context: 128K tokens (huge!)
           â€¢ Best for: Long documents, complex tasks
        
        4. ğŸƒâ€â™‚ï¸ GPT-3.5-turbo-instruct (The Classic!)
           â€¢ Traditional completion model
           â€¢ Good for: Simple text completion
           â€¢ Use when: You need completion, not conversation
        
        ğŸ’¡ Pro Tips for Model Selection:
        
        For Learning â†’ GPT-3.5-turbo (cheap and fast!)
        For Production Chatbots â†’ GPT-3.5-turbo
        For Complex Analysis â†’ GPT-4
        For Long Documents â†’ GPT-4-turbo
        For Simple Completion â†’ GPT-3.5-turbo-instruct
        
        Remember: Start with GPT-3.5-turbo, upgrade only when needed!
        """
        
        print(models_info)


def main():
    """
    Main function to run all LLM demonstrations
    
    Chalo shuru karte hain LLM ki detailed exploration! ğŸš€
    """
    print("ğŸ§  Welcome to LLM Deep Dive!")
    print("ğŸ‘¨â€ğŸ’» Your guide from tier-2 city is here to help!")
    print("ğŸ¯ Let's master LLMs and Chat Models together!\n")
    
    # Initialize our explorer
    explorer = LLMExplorer()
    
    # Run all demonstrations
    try:
        explorer.demonstrate_temperature_effects()
        explorer.llm_vs_chat_comparison()
        explorer.demonstrate_system_messages()
        explorer.token_limits_explanation()
        explorer.error_handling_demo()
        explorer.model_comparison_guide()
        
        # Final summary
        print("\n" + "="*60)
        print("ğŸŠ CONGRATULATIONS! You've mastered LLM basics!")
        print("="*60)
        print("âœ… You understand temperature effects")
        print("âœ… You know LLM vs Chat Model differences")
        print("âœ… You can use system messages for personality")
        print("âœ… You understand tokens and their importance")
        print("âœ… You can handle API errors gracefully")
        print("âœ… You can choose the right model for your needs")
        
        print("\nğŸ“š What's Next?")
        print("   â†’ 03_prompts_and_templates.py - Master the art of prompting!")
        print("   â†’ Experiment with different temperatures")
        print("   â†’ Try different models and compare results")
        print("   â†’ Practice error handling in your own projects")
        
        print("\nğŸ’¡ Key Takeaway:")
        print("   Temperature 0.7 is your sweet spot for most applications!")
        print("   GPT-3.5-turbo is perfect for learning and most production use cases!")
        
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Tutorial interrupted! Come back anytime!")
    except Exception as e:
        print(f"\nâŒ Unexpected error: {e}")
        print("ğŸ’¡ Check your internet connection and API key")


if __name__ == "__main__":
    # Let's explore the fascinating world of LLMs!
    # Chaliye LLMs ki duniya mein ghumte hain! ğŸŒŸ
    main()
