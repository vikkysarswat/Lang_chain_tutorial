#!/usr/bin/env python3
"""
ü§ñ RAG Implementation in LangChain - Build Your Own ChatGPT for Documents!

Author: Senior AI Developer from Tier-2 City, India
Purpose: Building production-ready RAG (Retrieval Augmented Generation) systems

Arre yaar! Ever wanted to build your own AI assistant that can answer questions
about YOUR specific documents? Like having a personal research assistant who has
read all your files and can instantly answer any question? That's RAG magic!

Think of RAG like this:
- Traditional AI = Smart student with general knowledge
- RAG = Smart student + access to your entire library
- It retrieves relevant info from YOUR docs, then generates answers
- Like having Jarvis from Iron Man, but for your documents!

What we'll master today:
1. RAG Architecture - Understanding the complete pipeline
2. Document Processing - Prepare your data like a pro
3. Embedding & Vector Store - Create searchable knowledge base
4. Retrieval Strategies - Find the most relevant information
5. Generation Pipeline - Create contextual answers
6. Advanced RAG Patterns - Multi-query, Re-ranking, Self-query
7. Production Optimization - Make it fast and reliable
8. Multilingual RAG - Handle Hindi + English content
9. Evaluation & Monitoring - Ensure quality answers

Real-world analogy: RAG is like having a super-smart librarian who can
instantly find relevant books and summarize the exact answer you need! üìö‚ú®

For complete implementation details and advanced patterns, visit:
https://python.langchain.com/docs/use_cases/question_answering/
"""

import os
import sys

# Add helpful startup message
print("üöÄ RAG Implementation Tutorial")
print("=" * 60)
print("This tutorial covers building production-ready RAG systems")
print("with LangChain for Indian developers.")
print()
print("üìö What You'll Learn:")
print("   ‚Ä¢ Complete RAG architecture and pipeline")
print("   ‚Ä¢ Document processing and chunking strategies")
print("   ‚Ä¢ Vector stores and embeddings")
print("   ‚Ä¢ Retrieval optimization techniques")
print("   ‚Ä¢ Multilingual support (Hindi + English)")
print("   ‚Ä¢ Production deployment best practices")
print("   ‚Ä¢ Evaluation and monitoring")
print()
print("üîë Prerequisites:")
print("   ‚Ä¢ Python 3.8+")
print("   ‚Ä¢ LangChain installed")
print("   ‚Ä¢ OpenAI API key (optional, can use alternatives)")
print("   ‚Ä¢ Basic understanding of LLMs")
print()
print("üí° Pro Tips:")
print("   ‚Ä¢ Start with small datasets to understand concepts")
print("   ‚Ä¢ Test with diverse queries in your domain")
print("   ‚Ä¢ Monitor performance metrics continuously")
print("   ‚Ä¢ Optimize based on actual user feedback")
print()
print("üáÆüá≥ For Indian Developers:")
print("   ‚Ä¢ Examples include Hindi + English content")
print("   ‚Ä¢ Focus on cost-effective solutions")
print("   ‚Ä¢ Tier-2 city deployment considerations")
print("   ‚Ä¢ Multilingual search optimization")
print()
print("=" * 60)
print()
print("üìñ Implementation Guide:")
print()
print("STEP 1: Document Ingestion")
print("-" * 40)
print("""
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Load documents
loader = PyPDFLoader("your_document.pdf")
documents = loader.load()

# Split into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\\n\\n", "\\n", ". ", " ", ""]
)
chunks = text_splitter.split_documents(documents)
""")

print("\nSTEP 2: Create Vector Store")
print("-" * 40)
print("""
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

# Initialize embeddings
embeddings = OpenAIEmbeddings()

# Create vector store
vector_store = FAISS.from_documents(
    documents=chunks,
    embedding=embeddings
)

# Save for later use
vector_store.save_local("vectorstore")
""")

print("\nSTEP 3: Build RAG Chain")
print("-" * 40)
print("""
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Initialize LLM
llm = ChatOpenAI(temperature=0.7, model="gpt-3.5-turbo")

# Create custom prompt
prompt_template = \"\"\"
Use the following context to answer the question.
If you don't know the answer, say so. Don't make up information.

Context: {context}

Question: {question}

Answer:\"\"\"

prompt = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)

# Create retriever
retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 4}
)

# Create QA chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt},
    return_source_documents=True
)
""")

print("\nSTEP 4: Query Your Documents")
print("-" * 40)
print("""
# Ask questions
result = qa_chain({"query": "What is LangChain?"})

print("Answer:", result["result"])
print("Sources:", len(result["source_documents"]))

for doc in result["source_documents"]:
    print(f"- {doc.metadata['source']}")
""")

print("\n" + "=" * 60)
print()
print("üîß Advanced Optimization Techniques:")
print()

print("1Ô∏è‚É£ Multi-Query Retrieval:")
print("-" * 40)
print("""
# Generate multiple query variations for better retrieval
from langchain.retrievers import MultiQueryRetriever

multi_query_retriever = MultiQueryRetriever.from_llm(
    retriever=vector_store.as_retriever(),
    llm=llm
)
""")

print("\n2Ô∏è‚É£ Contextual Compression:")
print("-" * 40)
print("""
# Compress retrieved documents to remove irrelevant information
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

compressor = LLMChainExtractor.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever
)
""")

print("\n3Ô∏è‚É£ Re-ranking:")
print("-" * 40)
print("""
# Re-rank retrieved documents for better relevance
# Use cross-encoder models for more accurate ranking
from langchain.retrievers import CohereRerank

reranker = CohereRerank(cohere_api_key="your-key")
rerank_retriever = reranker.rerank(
    query="your query",
    documents=retrieved_docs
)
""")

print("\n" + "=" * 60)
print()
print("üìä Evaluation Best Practices:")
print()

print("Key Metrics to Track:")
print("-" * 40)
print("""
1. Retrieval Metrics:
   ‚Ä¢ Precision@K: Relevant docs in top K results
   ‚Ä¢ Recall@K: % of relevant docs retrieved
   ‚Ä¢ MRR: Mean Reciprocal Rank
   ‚Ä¢ NDCG: Normalized Discounted Cumulative Gain

2. Generation Metrics:
   ‚Ä¢ Faithfulness: Answer accuracy vs source
   ‚Ä¢ Relevance: Answer relevance to question
   ‚Ä¢ Coherence: Answer quality and structure
   ‚Ä¢ Completeness: Full answer coverage

3. System Metrics:
   ‚Ä¢ Response time (latency)
   ‚Ä¢ Throughput (queries/second)
   ‚Ä¢ Success rate
   ‚Ä¢ Cost per query
   ‚Ä¢ User satisfaction

4. For Indian Applications:
   ‚Ä¢ Cross-lingual retrieval accuracy
   ‚Ä¢ Code-switching handling
   ‚Ä¢ Cultural context preservation
   ‚Ä¢ Regional language support
""")

print("\n" + "=" * 60)
print()
print("üáÆüá≥ Multilingual RAG for Indian Context:")
print()

print("Handling Hindi + English Content:")
print("-" * 40)
print("""
# Use multilingual embeddings
from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# Custom prompt for multilingual responses
multilingual_prompt = \"\"\"
‡§Ü‡§™ ‡§è‡§ï helpful assistant ‡§π‡•à‡§Ç‡•§ ‡§¶‡§ø‡§è ‡§ó‡§è context ‡§∏‡•á ‡§∏‡§µ‡§æ‡§≤ ‡§ï‡§æ ‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•á‡§Ç‡•§

Context: {context}

Question: {question}

Instructions:
- Answer in the same language as the question
- Use information from context only
- Be accurate and concise

Answer:\"\"\"
""")

print("\n" + "=" * 60)
print()
print("üöÄ Production Deployment Checklist:")
print()

checklist = {
    "Infrastructure": [
        "Choose deployment platform (AWS/GCP/Azure/On-premise)",
        "Set up auto-scaling for variable loads",
        "Configure load balancing",
        "Implement caching (Redis/Memcached)",
        "Set up monitoring and logging"
    ],
    "Performance": [
        "Optimize chunk size and overlap",
        "Use efficient vector index (FAISS/HNSW)",
        "Implement query caching",
        "Batch process similar queries",
        "Use CDN for static content"
    ],
    "Security": [
        "Secure API keys and credentials",
        "Implement rate limiting",
        "Add input validation and sanitization",
        "Use HTTPS for all communications",
        "Set up authentication and authorization"
    ],
    "Monitoring": [
        "Track query latency",
        "Monitor retrieval quality",
        "Log user queries and responses",
        "Set up alerting for failures",
        "Implement A/B testing framework"
    ],
    "Cost Optimization": [
        "Cache frequent queries",
        "Use efficient embedding models",
        "Optimize token usage",
        "Choose appropriate instance sizes",
        "Monitor and optimize API costs"
    ]
}

for category, items in checklist.items():
    print(f"‚úÖ {category}:")
    for item in items:
        print(f"   ‚Ä¢ {item}")
    print()

print("=" * 60)
print()
print("üí° Pro Tips for Indian Startups:")
print()
tips = [
    "Start with open-source models (HuggingFace) to reduce costs",
    "Use FAISS or Chroma for local vector storage",
    "Implement smart caching to minimize API calls",
    "Choose regional cloud instances for lower latency",
    "Test with actual user queries from your domain",
    "Build feedback loop for continuous improvement",
    "Consider hybrid deployment (local + cloud)",
    "Focus on Hindi + English support from day 1",
    "Join Indian AI communities for support",
    "Contribute back to open source when possible"
]

for i, tip in enumerate(tips, 1):
    print(f"{i:2}. {tip}")

print()
print("=" * 60)
print()
print("üìö Additional Resources:")
print()
print("üìñ Documentation:")
print("   ‚Ä¢ LangChain Docs: https://python.langchain.com/")
print("   ‚Ä¢ RAG Guide: https://python.langchain.com/docs/use_cases/question_answering/")
print("   ‚Ä¢ Vector Stores: https://python.langchain.com/docs/modules/data_connection/vectorstores/")
print()
print("üéì Tutorials:")
print("   ‚Ä¢ LangChain Crash Course")
print("   ‚Ä¢ Building Production RAG Systems")
print("   ‚Ä¢ Advanced Retrieval Techniques")
print()
print("üë• Community:")
print("   ‚Ä¢ LangChain Discord")
print("   ‚Ä¢ Indian AI Developer Communities")
print("   ‚Ä¢ Stack Overflow - langchain tag")
print()
print("=" * 60)
print()
print("üéâ Congratulations! You now understand RAG implementation!")
print()
print("Next Steps:")
print("1. Build a simple RAG system with your own documents")
print("2. Experiment with different chunk sizes and retrieval strategies")
print("3. Implement evaluation metrics for your use case")
print("4. Optimize for production based on real user feedback")
print("5. Check out agents and tools tutorial for even more power!")
print()
print("Happy Building! üöÄüáÆüá≥")
print("=" * 60)

# Example usage function
def run_rag_example():
    """
    Complete RAG implementation example
    
    This function demonstrates a full RAG pipeline from start to finish.
    Uncomment and modify for your specific use case.
    """
    
    print("\nüîß RAG Example Implementation")
    print("=" * 60)
    print()
    print("This is a template. To use:")
    print("1. Set your OPENAI_API_KEY environment variable")
    print("2. Replace 'your_documents/' with your document path")
    print("3. Customize the prompt template for your use case")
    print("4. Adjust chunk_size and k based on your needs")
    print()
    
    example_code = '''
# Import required packages
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import os

# Set your API key
os.environ["OPENAI_API_KEY"] = "your-api-key-here"

# 1. Load documents
print("Loading documents...")
loader = DirectoryLoader(
    "your_documents/",
    glob="**/*.txt",
    loader_cls=TextLoader
)
documents = loader.load()
print(f"Loaded {len(documents)} documents")

# 2. Split documents
print("Splitting documents...")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)
print(f"Created {len(chunks)} chunks")

# 3. Create embeddings and vector store
print("Creating vector store...")
embeddings = OpenAIEmbeddings()
vector_store = FAISS.from_documents(chunks, embeddings)
vector_store.save_local("my_vectorstore")
print("Vector store created and saved")

# 4. Create RAG chain
print("Setting up RAG chain...")
llm = ChatOpenAI(temperature=0.7, model="gpt-3.5-turbo")
retriever = vector_store.as_retriever(search_kwargs={"k": 4})

prompt_template = """
Use the context below to answer the question.
If you cannot answer from the context, say so.

Context: {context}

Question: {question}

Answer:"""

prompt = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt},
    return_source_documents=True
)

# 5. Query the system
print("\\nRAG System Ready! Ask questions:\\n")

# Example queries
questions = [
    "What is the main topic of the documents?",
    "Can you summarize the key points?",
    "What are the important concepts mentioned?"
]

for question in questions:
    print(f"Q: {question}")
    result = qa_chain({"query": question})
    print(f"A: {result['result']}")
    print(f"Sources: {len(result['source_documents'])} documents")
    print("-" * 60)
'''
    
    print(example_code)
    print()
    print("=" * 60)

if __name__ == "__main__":
    print("\nüí° This is an educational tutorial file.")
    print("To run RAG examples, use the provided code templates above.")
    print("\nFor interactive implementation, uncomment and modify the")
    print("run_rag_example() function.")
